{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actually get predicted grwoth for each merchant\n",
    "\n",
    "purpose:\n",
    "- run through predicted growth over all merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "growth = pd.read_csv(\"../data/curated/final_model/input/fortnightly_agg_merchant_transactions_NOTAKE_NOFRAUD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just get last week\n",
    "growth_final_week = growth[(growth['Year']==2022) & (growth['Fortnight'] == 16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "growth_train = pd.read_csv(\"../data/curated/final_model/input/fortnightly_agg_merchant_transactions_train_NOTAKE_NOFRAUD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just get last week\n",
    "growth_train_final_week = growth[(growth['Year']==2022) & (growth['Fortnight'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Er(growth_final_week):\n",
    "\n",
    "    # standardise\n",
    "    growth_standardised = pd.DataFrame()\n",
    "\n",
    "    missing_abn_list = list()\n",
    "\n",
    "    for abn in list(growth_final_week['merchant_abn']):\n",
    "\n",
    "        try:\n",
    "            with open(f'../models/growth_standardiser/sum_transactions/{abn}.pickle', 'rb') as g:\n",
    "                SS1 = pickle.load(g)\n",
    "\n",
    "            with open(f'../models/growth_standardiser/number_of_customers/{abn}.pickle', 'rb') as g:\n",
    "                SS2 = pickle.load(g)\n",
    "\n",
    "            with open(f'../models/growth_standardiser/distinct_customers/{abn}.pickle', 'rb') as g:\n",
    "                SS3 = pickle.load(g)\n",
    "\n",
    "        \n",
    "            dta = growth_final_week[growth_final_week['merchant_abn'] == abn]\n",
    "\n",
    "            dta['sum_transactions_STD'] = SS1.transform(dta[['sum_transactions']])\n",
    "            dta['number_of_customers_STD'] = SS2.transform(dta[['number_of_customers']])\n",
    "            dta['distinct_customers_STD'] = SS3.transform(dta[['distinct_customers']])\n",
    "\n",
    "            growth_standardised = pd.concat([growth_standardised, dta])\n",
    "        \n",
    "        except:\n",
    "            missing_abn_list.append(abn)\n",
    "\n",
    "\n",
    "    # use validate set to fit standardisers for certain merchants because they were nonexistent in training set\n",
    "    growth_fill_missing = pd.read_csv(\"../data/curated/final_model/input/fortnightly_agg_merchant_transactions_validate_NOTAKE_NOFRAUD.csv\")\n",
    "\n",
    "\n",
    "    for id, dta in growth_fill_missing.groupby(['merchant_abn']):\n",
    "        \n",
    "        if id in missing_abn_list:\n",
    "            # fit the StandardScaler using train model\n",
    "            SS1 = StandardScaler()\n",
    "            SS1.fit(dta[['sum_transactions']])\n",
    "            \n",
    "            SS2 = StandardScaler()\n",
    "            SS2.fit(dta[['number_of_customers']])\n",
    "\n",
    "            SS3 = StandardScaler()\n",
    "            SS3.fit(dta[['distinct_customers']])\n",
    "\n",
    "            # export the model\n",
    "            with open(f'../models/growth_standardiser/sum_transactions/{id}.pickle', 'wb') as f:\n",
    "                pickle.dump(SS1,f)\n",
    "            with open(f'../models/growth_standardiser/number_of_customers/{id}.pickle', 'wb') as f:\n",
    "                pickle.dump(SS2,f)\n",
    "            with open(f'../models/growth_standardiser/distinct_customers/{id}.pickle', 'wb') as f:\n",
    "                pickle.dump(SS3,f)\n",
    "\n",
    "\n",
    "    for abn in missing_abn_list:\n",
    "\n",
    "        with open(f'../models/growth_standardiser/sum_transactions/{abn}.pickle', 'rb') as g:\n",
    "            SS1 = pickle.load(g)\n",
    "\n",
    "        with open(f'../models/growth_standardiser/number_of_customers/{abn}.pickle', 'rb') as g:\n",
    "            SS2 = pickle.load(g)\n",
    "\n",
    "        with open(f'../models/growth_standardiser/distinct_customers/{abn}.pickle', 'rb') as g:\n",
    "            SS3 = pickle.load(g)\n",
    "\n",
    "\n",
    "        dta = growth_final_week[growth_final_week['merchant_abn'] == abn]\n",
    "\n",
    "        dta['sum_transactions_STD'] = SS1.transform(dta[['sum_transactions']])\n",
    "        dta['number_of_customers_STD'] = SS2.transform(dta[['number_of_customers']])\n",
    "        dta['distinct_customers_STD'] = SS3.transform(dta[['distinct_customers']])\n",
    "\n",
    "        growth_standardised = pd.concat([growth_standardised, dta])\n",
    "\n",
    "\n",
    "    # drop unstandardised columns\n",
    "    growth_standardised = growth_standardised.drop(['sum_transactions', 'number_of_customers', 'distinct_customers'], axis=1)\n",
    "    # rename original columns with t-0\n",
    "    growth_standardised = growth_standardised.rename(columns={'sum_transactions_STD': 'sum_transactions_STD_t-0', 'number_of_customers_STD': 'number_of_customers_STD_t-0', 'distinct_customers_STD': 'distinct_customers_STD_t-0'})\n",
    "\n",
    "\n",
    "    with open(f'../models/growth/delta_revenue.pickle', 'rb') as g:\n",
    "        LR1 = pickle.load(g)\n",
    "\n",
    "    with open(f'../models/growth/delta_number_of_customers.pickle', 'rb') as g:\n",
    "        LR2 = pickle.load(g)\n",
    "\n",
    "    with open(f'../models/growth/delta_distinct_customers.pickle', 'rb') as g:\n",
    "        LR3 = pickle.load(g)\n",
    "\n",
    "\n",
    "    # iteratively find growth for future T+t periods\n",
    "    for i in range(7):\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            growth_standardised, all_Er = run_one_prediction(growth_standardised, i, LR1, LR2, LR3, all_Er = None)\n",
    "        \n",
    "        else:\n",
    "            growth_standardised, all_Er = run_one_prediction(growth_standardised, i, LR1, LR2, LR3, all_Er = all_Er)\n",
    "\n",
    "    weighted_avgs = list()\n",
    "\n",
    "    for row in all_Er.iterrows():\n",
    "\n",
    "        weighted_avg = sum([row[1][i+1] * (1/(2**i+1)) for i in range(7)])\n",
    "        \n",
    "        weighted_avgs.append(weighted_avg)\n",
    "        \n",
    "        \n",
    "    # get final dataframe\n",
    "    final_Er = all_Er[['merchant_abn']]\n",
    "\n",
    "    final_Er['weighted E(r)'] = weighted_avgs\n",
    "\n",
    "    return final_Er"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the predictors for each of the three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_prediction(growth_standardised, i, LR1, LR2, LR3, all_Er = None):    \n",
    "    pred1 = LR1.predict(growth_standardised[['sum_transactions_STD_t-0', 'number_of_customers_STD_t-0', 'distinct_customers_STD_t-0']])\n",
    "    pred2 = LR2.predict(growth_standardised[['sum_transactions_STD_t-0', 'number_of_customers_STD_t-0', 'distinct_customers_STD_t-0']])\n",
    "    pred3 = LR3.predict(growth_standardised[['sum_transactions_STD_t-0', 'number_of_customers_STD_t-0', 'distinct_customers_STD_t-0']])\n",
    "\n",
    "    # get the delta values\n",
    "    growth_standardised['delta(sum_transactions)'] = pred1 \n",
    "    growth_standardised['delta(number_of_customers)'] = pred2\n",
    "    growth_standardised['delta(distinct_customers)'] = pred3\n",
    "\n",
    "    # get the t+1 values\n",
    "    growth_standardised['sum_transactions_STD_t+1'] = growth_standardised['sum_transactions_STD_t-0'] + growth_standardised['delta(sum_transactions)']\n",
    "    growth_standardised['number_of_customers_STD_t+1'] = growth_standardised['number_of_customers_STD_t-0'] + growth_standardised['delta(number_of_customers)']\n",
    "    growth_standardised['distinct_customers_STD_t+1'] = growth_standardised['distinct_customers_STD_t-0'] + growth_standardised['delta(distinct_customers)']\n",
    "\n",
    "    # de-standardise\n",
    "    growth_new = pd.DataFrame()\n",
    "\n",
    "    for abn in list(growth_standardised['merchant_abn']):\n",
    "\n",
    "        try:\n",
    "            with open(f'../models/growth_standardiser/sum_transactions/{abn}.pickle', 'rb') as g:\n",
    "                SS1 = pickle.load(g)\n",
    "\n",
    "        \n",
    "            dta = growth_standardised[growth_standardised['merchant_abn'] == abn]\n",
    "\n",
    "            dta['sum_transactions_t-0'] = SS1.inverse_transform(dta[['sum_transactions_STD_t-0']])\n",
    "\n",
    "            dta['sum_transactions_t+1'] = SS1.inverse_transform(dta[['sum_transactions_STD_t+1']])\n",
    "\n",
    "            growth_new = pd.concat([growth_new, dta])\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # get the percentage growth\n",
    "    growth_new['E(r)'] = (growth_new['sum_transactions_STD_t+1']-growth_new['sum_transactions_STD_t-0'])/growth_new['sum_transactions_STD_t-0']\n",
    "    # because growth at a minimum can only be -100%\n",
    "    growth_new['E(r)'] = growth_new['E(r)'].apply(lambda x:x if x >= -1 else -1)\n",
    "\n",
    "    # if first time running, then create a new dataframe\n",
    "    if all_Er is None:\n",
    "        all_Er = growth_new[['merchant_abn']]\n",
    "    \n",
    "    all_Er[f'E(r)_{i}'] = growth_new['E(r)']\n",
    "\n",
    "    # reset growth_standardised and return these files\n",
    "    growth_standardised = growth_new[['merchant_abn', 'sum_transactions_STD_t+1', 'number_of_customers_STD_t+1', 'distinct_customers_STD_t+1']]\n",
    "    growth_standardised = growth_standardised.rename(columns = {'sum_transactions_STD_t+1': 'sum_transactions_STD_t-0',\n",
    "    'number_of_customers_STD_t+1': 'number_of_customers_STD_t-0', 'distinct_customers_STD_t+1': 'distinct_customers_STD_t-0'})\n",
    "\n",
    "    return growth_standardised, all_Er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training set and final model's predicted future growths\n",
    "Er_final_model = get_Er(growth_final_week)\n",
    "Er_train_model = get_Er(growth_train_final_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "Er_final_model.to_csv(\"../data/curated/final_model/input/E(r)_full.csv\", index=False)\n",
    "Er_train_model.to_csv(\"../data/curated/final_model/input/E(r)_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
