{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 17:02:15 WARN Utils: Your hostname, Haiyangs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.12.175.225 instead (on interface en0)\n",
      "22/10/01 17:02:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 17:02:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"preprocessing of taxi data\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"15g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sdf = spark.read.option(\"header\", True).csv(\"2016_age.csv\")\n",
    "education_sdf = spark.read.option(\"header\", True).csv(\"2016_education.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/01 17:02:23 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>18_24</th><th>25_34</th><th>35_44</th><th>45_54</th><th>55_64</th><th>65_plus</th><th>sub_total</th><th>postcode</th></tr>\n",
       "<tr><td>0.2166044129539066</td><td>0.41118256382610036</td><td>0.1598059224459933</td><td>0.08290654241595749</td><td>0.06426893603912356</td><td>0.06523162231891871</td><td>25969.0</td><td>2000</td></tr>\n",
       "<tr><td>0.9347107438016529</td><td>0.045454545454545456</td><td>0.00743801652892562</td><td>0.004958677685950...</td><td>0.00743801652892562</td><td>0.0</td><td>1210.0</td><td>2006</td></tr>\n",
       "<tr><td>0.39983285577841454</td><td>0.3390639923591213</td><td>0.11926934097421203</td><td>0.05742597898758357</td><td>0.04035339063992359</td><td>0.04405444126074499</td><td>8376.0</td><td>2007</td></tr>\n",
       "<tr><td>0.39916003931730853</td><td>0.3594853006880529</td><td>0.10481637029756054</td><td>0.061388615852023945</td><td>0.042623536770619246</td><td>0.032526137074434816</td><td>11191.0</td><td>2008</td></tr>\n",
       "<tr><td>0.1143575515866135</td><td>0.3546904803190567</td><td>0.20469915033813074</td><td>0.11938616264955783</td><td>0.10499393098664817</td><td>0.10187272411999307</td><td>11534.0</td><td>2009</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+---------+--------+\n",
       "|              18_24|               25_34|              35_44|               45_54|               55_64|             65_plus|sub_total|postcode|\n",
       "+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+---------+--------+\n",
       "| 0.2166044129539066| 0.41118256382610036| 0.1598059224459933| 0.08290654241595749| 0.06426893603912356| 0.06523162231891871|  25969.0|    2000|\n",
       "| 0.9347107438016529|0.045454545454545456|0.00743801652892562|0.004958677685950...| 0.00743801652892562|                 0.0|   1210.0|    2006|\n",
       "|0.39983285577841454|  0.3390639923591213|0.11926934097421203| 0.05742597898758357| 0.04035339063992359| 0.04405444126074499|   8376.0|    2007|\n",
       "|0.39916003931730853|  0.3594853006880529|0.10481637029756054|0.061388615852023945|0.042623536770619246|0.032526137074434816|  11191.0|    2008|\n",
       "| 0.1143575515866135|  0.3546904803190567|0.20469915033813074| 0.11938616264955783| 0.10499393098664817| 0.10187272411999307|  11534.0|    2009|\n",
       "+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+---------+--------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"18_24\", \"25_34\", \"35_44\", \n",
    "           \"45_54\", \"55_64\", \"65_plus\", \n",
    "           \"sub_total\", \"postcode\"]\n",
    "\n",
    "age_group = columns[:-1]\n",
    "\n",
    "age_sdf = age_sdf.withColumn(columns[6], reduce(lambda x,y:x+y, [F.col(str(x)) for x in range(18, 116)])) \\\n",
    "                 .withColumn(columns[0], reduce(lambda x,y:x+y, [F.col(str(x)) for x in range(18, 25)]) / F.col(columns[6])) \\\n",
    "                 .withColumn(columns[1], reduce(lambda x,y:x+y, [F.col(str(x)) for x in range(25, 35)]) / F.col(columns[6])) \\\n",
    "                 .withColumn(columns[2], reduce(lambda x,y:x+y, [F.col(str(x)) for x in range(35, 45)]) / F.col(columns[6])) \\\n",
    "                 .withColumn(columns[3], reduce(lambda x,y:x+y, [F.col(str(x)) for x in range(45, 55)]) / F.col(columns[6])) \\\n",
    "                 .withColumn(columns[4], reduce(lambda x,y:x+y, [F.col(str(x)) for x in range(55, 65)]) / F.col(columns[6])) \\\n",
    "                 .withColumn(columns[5], reduce(lambda x,y:x+y, [F.col(str(x)) for x in range(65, 116)]) / F.col(columns[6])) \\\n",
    "                 .withColumn(columns[7], F.regexp_extract('AGEP Age', r'\\d+', 0)) \\\n",
    "                 .select(*columns)\n",
    "                \n",
    "age_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>postcode</th><th>total_parttime_or_fulltime_students</th></tr>\n",
       "<tr><td>2000</td><td>8101</td></tr>\n",
       "<tr><td>2006</td><td>1332</td></tr>\n",
       "<tr><td>2007</td><td>4008</td></tr>\n",
       "<tr><td>2008</td><td>5147</td></tr>\n",
       "<tr><td>2009</td><td>1726</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----------------------------------+\n",
       "|postcode|total_parttime_or_fulltime_students|\n",
       "+--------+-----------------------------------+\n",
       "|    2000|                               8101|\n",
       "|    2006|                               1332|\n",
       "|    2007|                               4008|\n",
       "|    2008|                               5147|\n",
       "|    2009|                               1726|\n",
       "+--------+-----------------------------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_sdf = education_sdf.withColumn(\"postcode\", F.regexp_extract(\"postcode\", r'\\d+', 0)) \\\n",
    "                           .withColumnRenamed(\"total\", \"total_parttime_or_fulltime_students\") \\\n",
    "                           .select(\"postcode\", \"total_parttime_or_fulltime_students\")\n",
    "\n",
    "student_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>postcode</th><th>median_salary</th></tr>\n",
       "<tr><td>2000</td><td>575.0</td></tr>\n",
       "<tr><td>2006</td><td>75.0</td></tr>\n",
       "<tr><td>2007</td><td>350.0</td></tr>\n",
       "<tr><td>2008</td><td>575.0</td></tr>\n",
       "<tr><td>2009</td><td>1125.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-------------+\n",
       "|postcode|median_salary|\n",
       "+--------+-------------+\n",
       "|    2000|        575.0|\n",
       "|    2006|         75.0|\n",
       "|    2007|        350.0|\n",
       "|    2008|        575.0|\n",
       "|    2009|       1125.0|\n",
       "+--------+-------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "income_sdf = spark.read.option(\"header\", True).csv(\"income_cleaned.csv\") \\\n",
    "                                              .select(\"postcode\", \"median_salary\")\n",
    "income_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(column_name):\n",
    "   return F.mean(column_name).alias(\"mean_\" + column_name)\n",
    "\n",
    "def processing_pipeline(dlist):\n",
    "    for data in dlist:\n",
    "        # Join persona data\n",
    "        data = data.join(income_sdf, \"postcode\", \"left\") \\\n",
    "                   .join(student_sdf, \"postcode\", \"left\") \\\n",
    "                   .join(age_sdf, \"postcode\", \"left\")\n",
    "\n",
    "        # remove rows contain null in persona\n",
    "        data = data.filter(F.col(\"median_salary\").isNotNull() \\\n",
    "                         | F.col(\"sub_total\").isNotNull() \\\n",
    "                         | F.col(\"total_parttime_or_fulltime_students\").isNotNull())\n",
    "\n",
    "        # take the mean of all persona\n",
    "        agg_data = data.groupBy(\"merchant_abn\") \\\n",
    "                       .agg(\n",
    "                            mean(\"median_salary\"),\n",
    "                            mean(\"total_parttime_or_fulltime_students\"),\n",
    "                            *[mean(name) for name in age_group]\n",
    "                            )\n",
    "        yield agg_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>merchant_abn</th><th>mean_median_salary</th><th>mean_total_parttime_or_fulltime_students</th><th>mean_18_24</th><th>mean_25_34</th><th>mean_35_44</th><th>mean_45_54</th><th>mean_55_64</th><th>mean_65_plus</th><th>mean_sub_total</th></tr>\n",
       "<tr><td>34440496342</td><td>665.3571428571429</td><td>726.0285714285715</td><td>0.09616178597603435</td><td>0.14638469969797188</td><td>0.15608870164698682</td><td>0.1735831056885002</td><td>0.20771864696238573</td><td>0.220063060028121</td><td>8046.771428571428</td></tr>\n",
       "<tr><td>15613631617</td><td>647.4765258215963</td><td>635.1502347417841</td><td>0.09355564743741474</td><td>0.14970744876770364</td><td>0.15654891609964974</td><td>0.17981347180449017</td><td>0.1928799103803407</td><td>0.22749460551040104</td><td>6140.830985915493</td></tr>\n",
       "<tr><td>83412691377</td><td>665.4689608636977</td><td>658.808439383791</td><td>0.10427259065156969</td><td>0.15142024706844687</td><td>0.15652496789966566</td><td>0.18378714204466093</td><td>0.1838697186090033</td><td>0.22012533372665358</td><td>6706.924426450742</td></tr>\n",
       "<tr><td>24406529929</td><td>661.9674887892377</td><td>613.1066666666667</td><td>0.09703010735768407</td><td>0.1508463018793074</td><td>0.15552580740901656</td><td>0.18518170229690945</td><td>0.18534575633732567</td><td>0.22607032471975688</td><td>6236.961883408072</td></tr>\n",
       "<tr><td>73256306726</td><td>660.6919642857143</td><td>703.9039145907474</td><td>0.09931809556504945</td><td>0.14988948980853775</td><td>0.1557895084700852</td><td>0.187349726576246</td><td>0.1821697535401777</td><td>0.22548342603990384</td><td>7043.998214285714</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+------------------+----------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+\n",
       "|merchant_abn|mean_median_salary|mean_total_parttime_or_fulltime_students|         mean_18_24|         mean_25_34|         mean_35_44|         mean_45_54|         mean_55_64|       mean_65_plus|   mean_sub_total|\n",
       "+------------+------------------+----------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+\n",
       "| 34440496342| 665.3571428571429|                       726.0285714285715|0.09616178597603435|0.14638469969797188|0.15608870164698682| 0.1735831056885002|0.20771864696238573|  0.220063060028121|8046.771428571428|\n",
       "| 15613631617| 647.4765258215963|                       635.1502347417841|0.09355564743741474|0.14970744876770364|0.15654891609964974|0.17981347180449017| 0.1928799103803407|0.22749460551040104|6140.830985915493|\n",
       "| 83412691377| 665.4689608636977|                        658.808439383791|0.10427259065156969|0.15142024706844687|0.15652496789966566|0.18378714204466093| 0.1838697186090033|0.22012533372665358|6706.924426450742|\n",
       "| 24406529929| 661.9674887892377|                       613.1066666666667|0.09703010735768407| 0.1508463018793074|0.15552580740901656|0.18518170229690945|0.18534575633732567|0.22607032471975688|6236.961883408072|\n",
       "| 73256306726| 660.6919642857143|                       703.9039145907474|0.09931809556504945|0.14988948980853775| 0.1557895084700852|  0.187349726576246| 0.1821697535401777|0.22548342603990384|7043.998214285714|\n",
       "+------------+------------------+----------------------------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-----------------+"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_sdf = spark.read.parquet(\"../data/curated/transactions_withoutfraud\") \\\n",
    "                            .drop('user_id2',\n",
    "                                  'order_datetime2',\n",
    "                                  'fraud rate',\n",
    "                                  'fraud2',\n",
    "                                  'user_id3',\n",
    "                                  'order_datetime3',\n",
    "                                  'fraud_probability',\n",
    "                                  'fraud3')\n",
    "\n",
    "# training data is the partition before the first fortnight of 2022\n",
    "train_transaction_sdf = transaction_sdf.where(F.col(\"order_datetime\") <= \"2022-1-16\")\n",
    "\n",
    "\n",
    "data_list = [transaction_sdf, train_transaction_sdf]\n",
    "\n",
    "data_list = list(processing_pipeline(data_list))\n",
    "\n",
    "data_list[0].limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_persona_score(dlist, filenames):\n",
    "    for data, filename in zip(dlist, filenames):\n",
    "        data = data.withColumn(\"pop_index\",\n",
    "                                0.26*F.col(\"mean_18_24\") + \n",
    "                                0.35*F.col(\"mean_25_34\") + \n",
    "                                0.20*F.col(\"mean_35_44\") + \n",
    "                                0.12*F.col(\"mean_45_54\") + \n",
    "                                0.05*F.col(\"mean_55_64\") +\n",
    "                                0.01*F.col(\"mean_65_plus\")\n",
    "                                )\n",
    "                                \n",
    "        data = data.select(\"merchant_abn\",\n",
    "                            \"mean_median_salary\", \n",
    "                            \"mean_total_parttime_or_fulltime_students\",\n",
    "                            \"pop_index\",\n",
    "                            \"mean_sub_total\")\n",
    "\n",
    "        for col in data.columns[1:]:\n",
    "            max_ = data.agg({col:\"max\"}).collect()[0][0]\n",
    "            min_ = data.agg({col:\"min\"}).collect()[0][0]\n",
    "            data = data.withColumn(col, normalization_(F.col(col), F.lit(max_), F.lit(min_)))\n",
    "            data = data.withColumnRenamed(col, \"normalized_\"+col)\n",
    "        \n",
    "\n",
    "        data = data.withColumn(\"persona_score\",\n",
    "                               F.col(\"normalized_mean_median_salary\") *\n",
    "                               F.col(\"normalized_mean_total_parttime_or_fulltime_students\") *\n",
    "                               F.col(\"normalized_pop_index\") *\n",
    "                               F.col(\"normalized_mean_sub_total\")\n",
    "                               )\n",
    "        \n",
    "        data.select(\"merchant_abn\", \"persona_score\").toPandas().to_csv(filename, index=False)\n",
    "        \n",
    "        print(\"%s is done\" % filename)\n",
    "\n",
    "        yield data\n",
    "        \n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "def normalization_(val, col_max, col_min):\n",
    "    val = float(val)\n",
    "    col_max = float(col_max)\n",
    "    col_min = float(col_min)\n",
    "    return (val - col_min) / (col_max - col_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_persona_score.csv is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained_persona_score.csv is done\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"full_persona_score.csv\", \"trained_persona_score.csv\"]\n",
    "list(compute_persona_score(data_list, filenames))[0].limit(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
