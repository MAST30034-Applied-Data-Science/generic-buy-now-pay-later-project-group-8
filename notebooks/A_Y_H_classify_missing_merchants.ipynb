{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ByteType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"preprocessing of taxi data\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of merchants\n",
    "### **Objective**: Predict product description, revenue level and take rate of the missing merchants\n",
    "### **Classification pipeline**:\n",
    " 0. Preliminary Data Analysis\n",
    " 1. Data Engineering\n",
    "  * Mostly done in ETL\n",
    "    * Encode revenue level into integer value, e.g. 1, 2, 3, 4, 5\n",
    "    * Clean the prod_desc (has been updated in ETL)\n",
    "  * Need one curated dataset for modeling product description and one dataset for modeling revenue level and take rate\n",
    " 2. Feature Engineering\n",
    "  * Aggregate data to produce more useful features for modeling revenue level and take rate\n",
    "  * Recommended features for prod_desc: dollar value, user id and order datetime\n",
    "  * Recommended features for revenue level and take rate: monthly average revenue,  monthly average number of orders, monthly average number of distinct customers, average revenue per order, median revenue, variance of dollar amount\n",
    " 3. Data Modeling\n",
    "  * Choice of classification model: XGBClassifier, RandomForest, Naive Bayes(Last resort)\n",
    "  * Choice of regression model: Linear regression, XGBregressor\n",
    "  * Fitting and Tuning model to achieve optimal performance \n",
    " 4. Model Validation\n",
    " * Metrics:\n",
    "    * Categorical(prod_desc and revenue_level): \n",
    "      * Accuracy\n",
    "      * f1 score\n",
    "    * Continuous(take_rate):\n",
    "      * RMSE\n",
    " * Visualization:\n",
    "    * Categorical:\n",
    "      * learning curve\n",
    "      * ROC curve\n",
    "      * confusion matrix \n",
    "    * Continuous:\n",
    "      * residuals vs. fitted value\n",
    " 5. Model deployment\n",
    "   * Use the prediction to impute missing information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Data Analysis\n",
    "#### Due to the limitation of my device, the PDA is performed on transaction between 2021/02/28 and 2021/08/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_sdf = spark.read.parquet(\"../data/curated/transactions_20210228_20210827\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_unknown_merchants = 378\n",
      "num_of_order_from_unknown_merchants = 149228\n"
     ]
    }
   ],
   "source": [
    "num_of_unknown_merchants = transaction_sdf.where(F.col(\"merchant_name\").isNull()) \\\n",
    "                            .select(F.col(\"merchant_abn\")).distinct().count()\n",
    "num_of_order_from_unknown_merchants = transaction_sdf.where(F.col(\"merchant_name\").isNull()) \\\n",
    "                            .select(F.col(\"merchant_abn\")).count()\n",
    "\n",
    "print(f\"num_of_unknown_merchants = {num_of_unknown_merchants}\" + \"\\n\" +\n",
    "      f\"num_of_order_from_unknown_merchants = {num_of_order_from_unknown_merchants}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if every missing merchant has at least one related known merchant, <u>i.e. if there is at least one customer who buys from an unknown merchant also buy from other merchants</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>unknown_merchant_abn</th><th>num_of_order_in_known_merchant</th><th>num_of_distinct_customers</th><th>num_of_known_merchant</th><th>num_of_distinct_prod_desc</th><th>num_of_distinct_revenue_level</th></tr>\n",
       "<tr><td>24406529929</td><td>151201</td><td>1021</td><td>3233</td><td>25</td><td>5</td></tr>\n",
       "<tr><td>56395390867</td><td>1287</td><td>9</td><td>587</td><td>24</td><td>5</td></tr>\n",
       "<tr><td>28767881738</td><td>136</td><td>1</td><td>116</td><td>22</td><td>4</td></tr>\n",
       "<tr><td>45925655949</td><td>3286</td><td>23</td><td>1000</td><td>24</td><td>5</td></tr>\n",
       "<tr><td>87802246756</td><td>116776</td><td>778</td><td>3092</td><td>25</td><td>5</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------------------------+-------------------------+---------------------+-------------------------+-----------------------------+\n",
       "|unknown_merchant_abn|num_of_order_in_known_merchant|num_of_distinct_customers|num_of_known_merchant|num_of_distinct_prod_desc|num_of_distinct_revenue_level|\n",
       "+--------------------+------------------------------+-------------------------+---------------------+-------------------------+-----------------------------+\n",
       "|         24406529929|                        151201|                     1021|                 3233|                       25|                            5|\n",
       "|         56395390867|                          1287|                        9|                  587|                       24|                            5|\n",
       "|         28767881738|                           136|                        1|                  116|                       22|                            4|\n",
       "|         45925655949|                          3286|                       23|                 1000|                       24|                            5|\n",
       "|         87802246756|                        116776|                      778|                 3092|                       25|                            5|\n",
       "+--------------------+------------------------------+-------------------------+---------------------+-------------------------+-----------------------------+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transaction_sdf.createOrReplaceTempView('transactions')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT merchant_abn, user_id\n",
    "FROM transactions\n",
    "WHERE merchant_name IS NULL\n",
    "\"\"\").createOrReplaceTempView('unknown_merchants')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM transactions\n",
    "WHERE merchant_name IS NOT NULL\n",
    "\"\"\").createOrReplaceTempView('orders_in_known_merchants')\n",
    "\n",
    "\n",
    "joined_sdf = spark.sql(\"\"\"\n",
    "SELECT table1.merchant_abn AS unknown_merchant_abn, \n",
    "    COUNT(table1.merchant_abn) AS num_of_order_in_known_merchant,\n",
    "    COUNT(DISTINCT table2.user_id) AS num_of_distinct_customers,\n",
    "    COUNT(DISTINCT table2.merchant_abn) AS num_of_known_merchant,\n",
    "    COUNT(DISTINCT table2.prod_desc) AS num_of_distinct_prod_desc,\n",
    "    COUNT(DISTINCT table2.revenue_level) AS num_of_distinct_revenue_level\n",
    "FROM unknown_merchants AS table1\n",
    "LEFT JOIN orders_in_known_merchants AS table2 \n",
    "ON table1.user_id=table2.user_id\n",
    "GROUP BY table1.merchant_abn\n",
    "\"\"\")\n",
    "\n",
    "joined_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>unknown_merchant_abn</th><th>num_of_order_in_known_merchant</th><th>num_of_distinct_customers</th><th>num_of_known_merchant</th><th>num_of_distinct_prod_desc</th><th>num_of_distinct_revenue_level</th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+------------------------------+-------------------------+---------------------+-------------------------+-----------------------------+\n",
       "|unknown_merchant_abn|num_of_order_in_known_merchant|num_of_distinct_customers|num_of_known_merchant|num_of_distinct_prod_desc|num_of_distinct_revenue_level|\n",
       "+--------------------+------------------------------+-------------------------+---------------------+-------------------------+-----------------------------+\n",
       "+--------------------+------------------------------+-------------------------+---------------------+-------------------------+-----------------------------+"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is at least one customer for every unknown merchant who purchase from a known merchant\n",
    "joined_sdf.filter(F.col(\"num_of_order_in_known_merchant\") == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of distinct product description\n",
    "transaction_sdf.select(F.col(\"prod_desc\")).distinct().orderBy(F.col(\"prod_desc\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>user_id</th><th>merchant_abn</th><th>dollar_value</th><th>order_id</th><th>order_datetime</th><th>merchant_name</th><th>prod_desc</th><th>revenue_level</th><th>take_rate</th><th>consumer_name</th><th>address</th><th>state</th><th>postcode</th><th>gender</th><th>consumer_id</th></tr>\n",
       "<tr><td>1389</td><td>10023283211</td><td>202.9580709315428</td><td>d79f7f72-1ee9-4d0...</td><td>2021-05-14</td><td>Felis Limited</td><td>furniture, home f...</td><td>e</td><td>0.1</td><td>Riley Dominguez</td><td>99661 Bradley Unions</td><td>ACT</td><td>2614</td><td>Male</td><td>1025254</td></tr>\n",
       "<tr><td>2000</td><td>10023283211</td><td>98.79635546902787</td><td>7122e4e3-1ff1-468...</td><td>2021-03-22</td><td>Felis Limited</td><td>furniture, home f...</td><td>e</td><td>0.1</td><td>Joshua Anderson</td><td>60145 Mendoza Hig...</td><td>NSW</td><td>2442</td><td>Male</td><td>1120280</td></tr>\n",
       "<tr><td>1495</td><td>10023283211</td><td>149.36202209710652</td><td>f29c1f95-491b-4b6...</td><td>2021-06-08</td><td>Felis Limited</td><td>furniture, home f...</td><td>e</td><td>0.1</td><td>Amanda Galloway</td><td>27887 Maria Drive...</td><td>NSW</td><td>2207</td><td>Female</td><td>386014</td></tr>\n",
       "<tr><td>1168</td><td>10023283211</td><td>354.4266795140579</td><td>5cfe6971-5b18-4e3...</td><td>2021-04-01</td><td>Felis Limited</td><td>furniture, home f...</td><td>e</td><td>0.1</td><td>David Sanders</td><td>09930 Angela Cove...</td><td>NSW</td><td>2021</td><td>Undisclosed</td><td>388087</td></tr>\n",
       "<tr><td>1759</td><td>10023283211</td><td>508.92006999805375</td><td>9c80bc74-e333-4e4...</td><td>2021-07-02</td><td>Felis Limited</td><td>furniture, home f...</td><td>e</td><td>0.1</td><td>Cynthia Pierce</td><td>6474 Kristina Bur...</td><td>VIC</td><td>3024</td><td>Female</td><td>599489</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+------------+------------------+--------------------+--------------+-------------+--------------------+-------------+---------+---------------+--------------------+-----+--------+-----------+-----------+\n",
       "|user_id|merchant_abn|      dollar_value|            order_id|order_datetime|merchant_name|           prod_desc|revenue_level|take_rate|  consumer_name|             address|state|postcode|     gender|consumer_id|\n",
       "+-------+------------+------------------+--------------------+--------------+-------------+--------------------+-------------+---------+---------------+--------------------+-----+--------+-----------+-----------+\n",
       "|   1389| 10023283211| 202.9580709315428|d79f7f72-1ee9-4d0...|    2021-05-14|Felis Limited|furniture, home f...|            e|      0.1|Riley Dominguez|99661 Bradley Unions|  ACT|    2614|       Male|    1025254|\n",
       "|   2000| 10023283211| 98.79635546902787|7122e4e3-1ff1-468...|    2021-03-22|Felis Limited|furniture, home f...|            e|      0.1|Joshua Anderson|60145 Mendoza Hig...|  NSW|    2442|       Male|    1120280|\n",
       "|   1495| 10023283211|149.36202209710652|f29c1f95-491b-4b6...|    2021-06-08|Felis Limited|furniture, home f...|            e|      0.1|Amanda Galloway|27887 Maria Drive...|  NSW|    2207|     Female|     386014|\n",
       "|   1168| 10023283211| 354.4266795140579|5cfe6971-5b18-4e3...|    2021-04-01|Felis Limited|furniture, home f...|            e|      0.1|  David Sanders|09930 Angela Cove...|  NSW|    2021|Undisclosed|     388087|\n",
       "|   1759| 10023283211|508.92006999805375|9c80bc74-e333-4e4...|    2021-07-02|Felis Limited|furniture, home f...|            e|      0.1| Cynthia Pierce|6474 Kristina Bur...|  VIC|    3024|     Female|     599489|\n",
       "+-------+------------+------------------+--------------------+--------------+-------------+--------------------+-------------+---------+---------------+--------------------+-----+--------+-----------+-----------+"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take_rate is the independent of the dollar amount of an order\n",
    "transaction_sdf.orderBy(F.col(\"merchant_abn\")).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 238:===================>                                     (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pearson's correlation coefficient between revenue level and take rate is -0.9520856896861268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Take rate and revenue level are highly negatively correlated\n",
    "\n",
    "revenue_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
    "\n",
    "@F.udf(returnType=ByteType())\n",
    "def revenue_level_converter(rl):\n",
    "    if rl:\n",
    "        return revenue_dict[rl]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pc = transaction_sdf.filter(F.col(\"revenue_level\").isNotNull()) \\\n",
    "                    .withColumn(\"revenue_level_int\", revenue_level_converter(F.col(\"revenue_level\"))) \\\n",
    "                    .corr('revenue_level_int', 'take_rate')\n",
    "print(\"The Pearson's correlation coefficient between revenue level and take rate is \" + str(pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "rmse = make_scorer(mean_squared_error, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"../data/curated/agg_transaction_train.csv\")\n",
    "pred_set = pd.read_csv(\"../data/curated/agg_transaction_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.iloc[:, 1:] # drop merchant_abn\n",
    "pred_set = pred_set.drop(columns=[\"merchant_abn\", \"take_rate\", \"revenue_level_int\"]) # drop merchant_abn\n",
    "train_set = train_set.fillna(0) # columns of stddev contains NULL if the merchant has only one transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set.drop(columns=[\"take_rate\", \"revenue_level_int\"])\n",
    "y_revenue_level = train_set[\"revenue_level_int\"]\n",
    "y_revenue_level = LabelEncoder().fit_transform(y_revenue_level)\n",
    "y_take_rate = train_set[\"take_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification of revenue level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39884981226533167"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "cross_val_score(dummy_clf, X, y_revenue_level, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3690904255319149"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(\n",
    " eta=0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0.1,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softmax',\n",
    " nthread=4,\n",
    " num_class = 5,\n",
    " seed=2022)\n",
    "\n",
    "cross_val_score(xgb_clf, X, y_revenue_level, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4133554443053818"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(max_depth=5, random_state=2022)\n",
    "cross_val_score(rf_clf, X, y_revenue_level, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression of take rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9148727289998918"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_reg = DummyRegressor(strategy=\"mean\")\n",
    "cross_val_score(dummy_reg, X, y_revenue_level, scoring = rmse, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9839411849355182"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "    eta=0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    nthread=4,\n",
    "    seed=2022)\n",
    "\n",
    "cross_val_score(xgb_reg, X, y_revenue_level, scoring=rmse, cv=5).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9133982666245025"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "cross_val_score(lin_reg, X, y_revenue_level, scoring=rmse, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification of prod_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classification model is choosen because of its high efficiency\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:26:48 WARN DAGScheduler: Broadcasting large task binary with size 1580.0 KiB\n",
      "22/10/04 23:26:49 WARN DAGScheduler: Broadcasting large task binary with size 1580.0 KiB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>features</th><th>prod_desc_idx</th></tr>\n",
       "<tr><td>(24321,[16,24081,...</td><td>3.0</td></tr>\n",
       "<tr><td>(24321,[16,24081,...</td><td>6.0</td></tr>\n",
       "<tr><td>(24321,[16,24081,...</td><td>10.0</td></tr>\n",
       "<tr><td>(24321,[16,24081,...</td><td>17.0</td></tr>\n",
       "<tr><td>(24321,[16,24081,...</td><td>2.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+-------------+\n",
       "|            features|prod_desc_idx|\n",
       "+--------------------+-------------+\n",
       "|(24321,[16,24081,...|          3.0|\n",
       "|(24321,[16,24081,...|          6.0|\n",
       "|(24321,[16,24081,...|         10.0|\n",
       "|(24321,[16,24081,...|         17.0|\n",
       "|(24321,[16,24081,...|          2.0|\n",
       "+--------------------+-------------+"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def col_indexer(cols, data):\n",
    "    \"\"\"\n",
    "    return transformed data where col(s) is mapped to an index\n",
    "    parameters:\n",
    "               cols (list or other iterables): a list a columns \n",
    "               data (spark dataframe): a spark dataframe of target data\n",
    "    \"\"\"\n",
    "    data = StringIndexer(inputCol=cols[0], outputCol=cols[0]+\"_idx\").fit(data).transform(data)\n",
    "    if len(cols) > 1:\n",
    "        return col_indexer(cols[1:], data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def col_ohe(col, data):\n",
    "    \"\"\"\n",
    "    return transformed data where col(s) is turned into a k-dimension vector, \n",
    "    k is the number of unique value of col\n",
    "    parameters:\n",
    "            cols (list or other iterables): a list a columns \n",
    "            data (spark dataframe): a spark dataframe of target data\n",
    "    \"\"\"\n",
    "    data = OneHotEncoder(inputCol=col[0], outputCol=col[0]+\"_vector\").fit(data).transform(data)\n",
    "    if len(col) > 1:\n",
    "        return col_ohe(col[1:], data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    \"user_id_vector\", \"dollar_value\",\n",
    "    \"order_doy_vector\",\n",
    "], outputCol='features')\n",
    "\n",
    "\n",
    "\n",
    "modified_transaction_sdf = transaction_sdf.filter(F.col(\"merchant_name\").isNotNull()) \\\n",
    "                                                  .withColumn(\"order_doy\", F.dayofyear(\"order_datetime\"))\n",
    "ohe_transaction_sdf = col_ohe([\"user_id\", \"order_doy\"], data=modified_transaction_sdf)\n",
    "idx_transaction_sdf = col_indexer([\"prod_desc\"], data=ohe_transaction_sdf)\n",
    "assembled_transaction_sdf = assembler.transform(idx_transaction_sdf).select(\"features\", \"prod_desc_idx\")\n",
    "assembled_transaction_sdf.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:26:49 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 253:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:26:56 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 23:26:57 WARN DAGScheduler: Broadcasting large task binary with size 7.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 256:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.10486449201320018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_set, test_set = assembled_transaction_sdf.randomSplit([0.8, 0.2], seed=2022)\n",
    "nb = NaiveBayes(smoothing=1.0, \n",
    "                modelType=\"multinomial\", \n",
    "                labelCol=\"prod_desc_idx\").fit(train_set)\n",
    "\n",
    "prediction = nb.transform(test_set)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"prod_desc_idx\",\n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "#### Since all the models have poor performance in terms of accuracy/RMSE, it is not feasible to deploy them in an imputation process. From our perspectives, tuning the model will be ultimately a waste of time because it will only refine our model instead of improving it significantly. <u>Therefore, we decide to leave these merchants as unknown for now, and we will try to use clustering method to impute their take rates in the future.</u>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
